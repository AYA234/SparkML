{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for ML\n",
    "And fit linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark=SparkSession.builder.appName('recipes').getOrCreate()\n",
    "\n",
    "df=spark.read.csv('./data/7_epi_r.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    return re.sub('[^a-zA-Z0-9]','_',name)\n",
    "\n",
    "cleand_names=[ clean_name(col) for col in df.columns]\n",
    "df=df.toDF(*cleand_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanning the cakeweek and wastless columns.\n",
    "df=df.filter((df['_cakeweek'].isNotNull() )  | (df['_cakeweek']>1 ) | (df['_cakeweek'] > 1 ) | (df['_cakeweek'] < 0))\n",
    "df=df.filter((df['_wasteless'].isNotNull() )  | (df['_wasteless']>1 ) | (df['_wasteless'] > 1 ) | (df['_wasteless'] < 0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIERS = [\"title\"] \n",
    "CONTINUOUS_COLUMNS = [ \"rating\", \"calories\", \"protein\", \"fat\", \"sodium\", ] \n",
    "TARGET_COLUMN = [\"dessert\"] \n",
    "BINARY_COLUMNS = [ x for x in df.columns if x not in CONTINUOUS_COLUMNS and x not in TARGET_COLUMN and x not in IDENTIFIERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with null values at the target colunn.\n",
    "df=df.dropna(subset=TARGET_COLUMN)\n",
    "# Drop the rows with null values as needed.\n",
    "df=df.dropna(subset= CONTINUOUS_COLUMNS + BINARY_COLUMNS, how ='all')\n",
    "\n",
    "# Fill the null binary values with 0.\n",
    "df=df.fillna(subset=BINARY_COLUMNS,value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the continuous values to int type\n",
    "for col in df.columns:\n",
    "    df=df.withColumn(col,df[col].cast('int'))\n",
    "\n",
    "df=df.fillna(subset=CONTINUOUS_COLUMNS,value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for the given percentage\n",
    "total_rows = df.count()\n",
    "threshold_percentage = 0.9995  # 99.95%\n",
    "threshold_row_count = int(total_rows * threshold_percentage)\n",
    "\n",
    "# Identify columns to drop\n",
    "columns_to_drop = []\n",
    "for column in df.columns:\n",
    "    value_counts = df.groupBy(column).count().collect()\n",
    "    same_value_count = sum(1 for count in value_counts if count[\"count\"] >= threshold_row_count)\n",
    "    if same_value_count == 1:  # If only one value appears frequently, drop the column\n",
    "        columns_to_drop.append(column)\n",
    "\n",
    "# Drop identified columns\n",
    "df_cleaned = df.drop(*columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the continuous columms to detect the outliers.\n",
    "for col in CONTINUOUS_COLUMNS:\n",
    "    df.select(col).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Removing outliers -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "high_percentile_values = df.approxQuantile(CONTINUOUS_COLUMNS, [0.99], 0.01)[0]\n",
    "low_percentile_values = df.approxQuantile(CONTINUOUS_COLUMNS, [0.11], 0.99)[0]\n",
    "\n",
    "\n",
    "\n",
    "for col_name, high_val, low_val in zip(CONTINUOUS_COLUMNS, high_percentile_values, low_percentile_values):\n",
    "    df = df.withColumn(col_name, \n",
    "                       when(col(col_name) > high_val, high_val)\n",
    "                       .when(col(col_name) < low_val, low_val)\n",
    "                       .otherwise(col(col_name)))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute null values & Normalize the data using pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Imputing null values -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer,VectorAssembler,MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create an imputer to fill the null values wiht the avarage.\n",
    "imputer = Imputer(\n",
    "    inputCols=CONTINUOUS_COLUMNS,\n",
    "    outputCols=[col_name  for col_name in CONTINUOUS_COLUMNS]\n",
    ")\n",
    "\n",
    "# In order to scale the data we have to assemble the continuous features to one column.\n",
    "continuous_feature_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_COLUMNS,\n",
    "    outputCol=\"continues_features\"\n",
    ")\n",
    "\n",
    "# Assemble the binary columns\n",
    "binary_features_assembler=VectorAssembler(\n",
    "    inputCols=BINARY_COLUMNS,\n",
    "    outputCol='binary_features'\n",
    ")\n",
    "\n",
    "# Create a scaler to normalize the data.\n",
    "continuous_scaler = MinMaxScaler(inputCol=\"continues_features\", outputCol=\"scaled_continuous_features\")\n",
    "\n",
    "# Assemble all the features togather\n",
    "feature_assembler=VectorAssembler(\n",
    "    inputCols=['scaled_continuous_features','binary_features'],\n",
    "    outputCol='features'\n",
    ")\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[imputer,continuous_feature_assembler,binary_features_assembler, continuous_scaler,feature_assembler])\n",
    "\n",
    "# Fit and transform the pipeline to scale the continuous columns.\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_scaled = pipeline_model.transform(df)\n",
    "\n",
    "# df_scaled.show()\n",
    "\n",
    "df_scaled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|dessert|\n",
      "+--------------------+-------+\n",
      "|(678,[0,1,2,3,4,2...|      0|\n",
      "|(678,[0,1,2,3,4,3...|      0|\n",
      "|(678,[0,1,2,3,4,1...|      0|\n",
      "|(678,[0,62,176,18...|      0|\n",
      "|(678,[0,1,2,3,4,3...|      0|\n",
      "|(678,[0,1,2,3,4,3...|      0|\n",
      "|(678,[0,29,60,62,...|      0|\n",
      "|(678,[0,139,196,2...|      0|\n",
      "|(678,[0,1,2,3,4,4...|      0|\n",
      "|(678,[0,1,2,3,4,1...|      0|\n",
      "|(678,[0,1,2,3,4,6...|      0|\n",
      "|(678,[0,71,121,14...|      0|\n",
      "|(678,[0,1,2,3,4,3...|      1|\n",
      "|(678,[0,1,2,3,4,4...|      0|\n",
      "|(678,[0,1,2,3,4,6...|      0|\n",
      "|(678,[0,1,2,3,4,1...|      0|\n",
      "|(678,[0,1,2,3,4,3...|      1|\n",
      "|(678,[0,1,2,3,4,6...|      0|\n",
      "|(678,[0,1,2,3,4,6...|      0|\n",
      "|(678,[0,1,2,3,4,2...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.select('features','dessert').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(678, {0: 0.4, 1: 0.0, 2: 0.0001, 3: 0.0, 4: 0.0, 22: 1.0, 43: 1.0, 153: 1.0, 232: 1.0, 309: 1.0, 331: 1.0, 332: 1.0, 541: 1.0, 625: 1.0, 639: 1.0, 677: 1.0}), dessert=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "df_vectored=df_scaled.select('features','dessert')\n",
    "df_vectored.head()\n",
    "cor=Correlation.corr(df_vectored,'feautres').collect()[0][0]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train_set,test_set=df_vectored.randomSplit([0.75,0.25])\n",
    "linreg=LinearRegression(featuresCol='features',labelCol='dessert')\n",
    "linreg=linreg.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----------+\n",
      "|            features|dessert|prediction|\n",
      "+--------------------+-------+----------+\n",
      "|(678,[0,1,2,3,4,7...|      0|         0|\n",
      "|(678,[0,1,2,3,4,7...|      0|         0|\n",
      "|(678,[0,1,2,3,4,9...|      0|         0|\n",
      "|(678,[0,1,2,3,4,9...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      1|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------+----------+\n",
      "|            features|dessert|prediction|\n",
      "+--------------------+-------+----------+\n",
      "|(678,[0,1,2,3,4,5...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      1|         1|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "|(678,[0,1,2,3,4,1...|      0|         0|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linreg.coefficients\n",
    "# Train results:\n",
    "res=linreg.evaluate(train_set)\n",
    "preds_test=res.predictions.withColumn('prediction',when(col('prediction')>0.5 ,1).otherwise(0))\n",
    "preds_test.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Test results:\n",
    "res=linreg.evaluate(test_set)\n",
    "preds_train=res.predictions.withColumn('prediction',when(col('prediction')>0.5 ,1).otherwise(0))\n",
    "preds_train.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|dessert|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|         0|  134|\n",
      "|      1|         1|  764|\n",
      "|      0|         0| 4105|\n",
      "|      0|         1|   35|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrix for train set\n",
    "train_confusion_matrix = preds_train.groupBy(\"dessert\", \"prediction\").count()\n",
    "train_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|dessert|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|         0|  317|\n",
      "|      1|         1| 2358|\n",
      "|      0|         0|12227|\n",
      "|      0|         1|  111|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrix for test set\n",
    "test_confusion_matrix = preds_test.groupBy(\"dessert\", \"prediction\").count()\n",
    "test_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision: 0.9561952440550688\n",
      "Train Recall: 0.8507795100222717\n",
      "Test Precision: 0.9550425273390036\n",
      "Test Recall: 0.8814953271028038\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall for train set\n",
    "tp = train_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fp = train_confusion_matrix.filter((col(\"dessert\") == 0) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fn = train_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 0)).first()[\"count\"]\n",
    "\n",
    "precision_train = tp / (tp + fp)\n",
    "recall_train = tp / (tp + fn)\n",
    "\n",
    "print(\"Train Precision:\", precision_train)\n",
    "print(\"Train Recall:\", recall_train)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate precision and recall for test set\n",
    "tp = test_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fp = test_confusion_matrix.filter((col(\"dessert\") == 0) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fn = test_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 0)).first()[\"count\"]\n",
    "\n",
    "precision_test = tp / (tp + fp)\n",
    "recall_test = tp / (tp + fn)\n",
    "\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add combined features- \n",
    "The same proccess with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----------+\n",
      "|            features|dessert|prediction|\n",
      "+--------------------+-------+----------+\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         0|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------+----------+\n",
      "|            features|dessert|prediction|\n",
      "+--------------------+-------+----------+\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      0|         0|\n",
      "|(680,[0,1,2,3,4,5...|      1|         1|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+----------+-----+\n",
      "|dessert|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|         0|  108|\n",
      "|      1|         1|  755|\n",
      "|      0|         0| 4064|\n",
      "|      0|         1|   46|\n",
      "+-------+----------+-----+\n",
      "\n",
      "+-------+----------+-----+\n",
      "|dessert|prediction|count|\n",
      "+-------+----------+-----+\n",
      "|      1|         0|  301|\n",
      "|      1|         1| 2409|\n",
      "|      0|         0|12249|\n",
      "|      0|         1|  119|\n",
      "+-------+----------+-----+\n",
      "\n",
      "Train Precision: 0.9425717852684145\n",
      "Train Recall: 0.8748551564310545\n",
      "Test Precision: 0.9529272151898734\n",
      "Test Recall: 0.8889298892988929\n"
     ]
    }
   ],
   "source": [
    "# Add the combined features to the df\n",
    "df=df.withColumn('protein_ratio',df['protein']*4/df['calories'])\n",
    "df=df.withColumn('fat_ratio',df['fat']*9/df['calories'])\n",
    "\n",
    "# Add the combined column names to the list\n",
    "CONTINUOUS_COLUMNS.append('fat_ratio')\n",
    "CONTINUOUS_COLUMNS.append('protein_ratio')\n",
    "\n",
    "# Impute and scale the data (again for the combined features)\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=CONTINUOUS_COLUMNS,\n",
    "    outputCols=[col_name  for col_name in CONTINUOUS_COLUMNS]\n",
    ")\n",
    "\n",
    "continuous_feature_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_COLUMNS,\n",
    "    outputCol=\"continues_features\"\n",
    ")\n",
    "\n",
    "binary_features_assembler=VectorAssembler(\n",
    "    inputCols=BINARY_COLUMNS,\n",
    "    outputCol='binary_features'\n",
    ")\n",
    "\n",
    "continuous_scaler = MinMaxScaler(inputCol=\"continues_features\", outputCol=\"scaled_continuous_features\")\n",
    "\n",
    "feature_assembler=VectorAssembler(\n",
    "    inputCols=['scaled_continuous_features','binary_features'],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[imputer,continuous_feature_assembler,binary_features_assembler, continuous_scaler,feature_assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_scaled = pipeline_model.transform(df)\n",
    "\n",
    "df_vectored=df_scaled.select('features','dessert')\n",
    "\n",
    "# Fit the linear regression model\n",
    "train_set,test_set=df_vectored.randomSplit([0.75,0.25])\n",
    "linreg=LinearRegression(featuresCol='features',labelCol='dessert')\n",
    "linreg=linreg.fit(train_set)\n",
    "\n",
    "# Measure the results\n",
    "# Train results:\n",
    "res=linreg.evaluate(train_set)\n",
    "preds_test=res.predictions.withColumn('prediction',when(col('prediction')>0.5 ,1).otherwise(0))\n",
    "preds_test.show()\n",
    "\n",
    "# Test results:\n",
    "res=linreg.evaluate(test_set)\n",
    "preds_train=res.predictions.withColumn('prediction',when(col('prediction')>0.5 ,1).otherwise(0))\n",
    "preds_train.show()\n",
    "\n",
    "# Confusion matrix for train and test:\n",
    "# Calculate confusion matrix for train set\n",
    "train_confusion_matrix = preds_train.groupBy(\"dessert\", \"prediction\").count()\n",
    "train_confusion_matrix.show()\n",
    "\n",
    "# Calculate confusion matrix for test set\n",
    "test_confusion_matrix = preds_test.groupBy(\"dessert\", \"prediction\").count()\n",
    "test_confusion_matrix.show()\n",
    "\n",
    "# Calculate precision and recall for train set\n",
    "tp = train_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fp = train_confusion_matrix.filter((col(\"dessert\") == 0) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fn = train_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 0)).first()[\"count\"]\n",
    "\n",
    "precision_train = tp / (tp + fp)\n",
    "recall_train = tp / (tp + fn)\n",
    "\n",
    "print(\"Train Precision:\", precision_train)\n",
    "print(\"Train Recall:\", recall_train)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate precision and recall for test set\n",
    "tp = test_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fp = test_confusion_matrix.filter((col(\"dessert\") == 0) & (col(\"prediction\") == 1)).first()[\"count\"]\n",
    "fn = test_confusion_matrix.filter((col(\"dessert\") == 1) & (col(\"prediction\") == 0)).first()[\"count\"]\n",
    "\n",
    "precision_test = tp / (tp + fp)\n",
    "recall_test = tp / (tp + fn)\n",
    "\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
